# PySpark Guide: Cumulative Sum, Window Frame (`rowsBetween`), and User Defined Functions (UDF)

---

## Overview

This README explains three important PySpark concepts in a **simple and clear** way:

1. **Cumulative Sum (Running Total)** using Window Functions
2. **Window Frame** using `rowsBetween()`
3. **User Defined Functions (UDF)** in PySpark

You will learn:

* What each concept means
* Why we use it
* How it works internally (basic idea)
* Real-world scenarios
* Step-by-step code examples
* Important interview points
* Best practices

---

# 1. Cumulative Sum (Running Total)

## What is Cumulative Sum?

**Cumulative Sum** means:

> Adding values row by row and keeping the running total.

Example:

| Day | Sales |
| --- | ----- |
| 1   | 100   |
| 2   | 200   |
| 3   | 50    |

Cumulative Sum:

| Day | Sales | Running Total |
| --- | ----- | ------------- |
| 1   | 100   | 100           |
| 2   | 200   | 300           |
| 3   | 50    | 350           |

---

## Why Do We Use Cumulative Sum?

Common business use cases:

* Sales running total
* Monthly expense tracking
* Profit growth
* Website visitors growth
* Inventory movement

---

## How PySpark Calculates Cumulative Sum?

Using **Window Functions**.

We define:

* Partition (optional)
* Order
* Window Frame

---

## Basic Syntax

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import sum, col

window_spec = Window.orderBy("Day")

df.withColumn(
    "Running_Total",
    sum("Sales").over(window_spec)
)
```

---

## Example Dataset

```python
data = [
    (1, 100),
    (2, 200),
    (3, 50)
]

columns = ["Day", "Sales"]

df = spark.createDataFrame(data, columns)
```

---

## Cumulative Sum Example

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import sum

window_spec = Window.orderBy("Day")

result_df = df.withColumn(
    "Running_Total",
    sum("Sales").over(window_spec)
)

result_df.display()
```

---

## Partition-wise Cumulative Sum

Used when we want separate running totals per group.

Example: Running total per store.

```python
window_spec = Window.partitionBy("Store").orderBy("Day")

result_df = df.withColumn(
    "Running_Total",
    sum("Sales").over(window_spec)
)
```

---

## Important Interview Points

* Cumulative Sum is implemented using **Window Function**
* Order is mandatory
* Without order, cumulative logic is meaningless
* Works without reducing rows (unlike groupBy)

---

# 2. Window Frame (`rowsBetween`)

## What is Window Frame?

Window Frame defines:

> Which rows should be included in the calculation.

It controls the **range of rows** used by the window function.

---

## Why Do We Need Window Frame?

To calculate:

* Moving Average
* Last 3 rows sum
* Previous row comparison
* Rolling metrics

---

## Types of Window Frames

| Type           | Meaning                |
| -------------- | ---------------------- |
| `rowsBetween`  | Based on row positions |
| `rangeBetween` | Based on value range   |

---

## `rowsBetween` Syntax

```python
window_spec = Window.orderBy("Day").rowsBetween(start, end)
```

---

## Special Values

| Value                       | Meaning        |
| --------------------------- | -------------- |
| `Window.unboundedPreceding` | From first row |
| `Window.unboundedFollowing` | Till last row  |
| `0`                         | Current row    |
| `-1`                        | Previous row   |
| `1`                         | Next row       |

---

## Example 1 — Cumulative Sum Using Window Frame

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import sum

window_spec = Window.orderBy("Day").rowsBetween(
    Window.unboundedPreceding,
    Window.currentRow
)

result_df = df.withColumn(
    "Running_Total",
    sum("Sales").over(window_spec)
)
```

Meaning:

* Start from first row
* End at current row

---

## Example 2 — Last 2 Rows Sum (Rolling Window)

```python
window_spec = Window.orderBy("Day").rowsBetween(-1, 0)

result_df = df.withColumn(
    "Last_2_Row_Sum",
    sum("Sales").over(window_spec)
)
```

Meaning:

* Previous row + current row

---

## Example 3 — Next 2 Rows Sum

```python
window_spec = Window.orderBy("Day").rowsBetween(0, 2)

result_df = df.withColumn(
    "Next_2_Row_Sum",
    sum("Sales").over(window_spec)
)
```

---

## Real-World Uses

* Stock price moving average
* Demand forecasting
* Sensor data smoothing
* Fraud pattern detection

---

## Important Interview Points

* Window Frame controls calculation boundaries
* Default frame is: `unboundedPreceding` to `currentRow`
* `rowsBetween` is position-based
* Needed for rolling calculations

---

# 3. User Defined Functions (UDF) in PySpark

## What is UDF?

**UDF (User Defined Function)** allows you to:

> Create your own custom function and use it on DataFrame columns.

---

## Why Do We Use UDF?

When built-in PySpark functions are not enough.

Examples:

* Custom text cleaning
* Complex business rules
* Custom scoring logic
* Special formatting

---

## Basic Flow

1. Create Python function
2. Convert into UDF
3. Apply on column

---

## Simple UDF Example

```python
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType


def categorize_sales(value):
    if value < 100:
        return "Low"
    elif value < 300:
        return "Medium"
    else:
        return "High"

udf_func = udf(categorize_sales, StringType())

result_df = df.withColumn(
    "Sales_Category",
    udf_func("Sales")
)
```

---

## UDF with Multiple Columns

```python
from pyspark.sql.types import IntegerType


def total_price(qty, price):
    return qty * price

udf_total = udf(total_price, IntegerType())

result_df = df.withColumn(
    "Total",
    udf_total("Quantity", "Price")
)
```

---

## Performance Warning

UDF is **slower** than built-in functions because:

* Runs in Python
* Breaks Spark optimization (Catalyst Optimizer)

---

## Better Alternative

Use built-in functions whenever possible:

```python
from pyspark.sql.functions import when

result_df = df.withColumn(
    "Sales_Category",
    when(col("Sales") < 100, "Low")
    .when(col("Sales") < 300, "Medium")
    .otherwise("High")
)
```

---

## Types of UDF

| Type       | Description        |
| ---------- | ------------------ |
| Python UDF | Standard UDF       |
| Pandas UDF | Faster, vectorized |
| Scala UDF  | Fastest            |

---

## Pandas UDF Example (Vectorized)

```python
from pyspark.sql.functions import pandas_udf
from pyspark.sql.types import LongType

@pandas_udf(LongType())
def multiply_by_two(s):
    return s * 2

result_df = df.withColumn("Double_Sales", multiply_by_two("Sales"))
```

---

## Important Interview Points

* UDF = custom logic
* Slower than built-in functions
* Prefer Pandas UDF for performance
* Avoid UDF in large production pipelines if possible

---

# Quick Comparison

| Concept        | Used For         | Key Function    |
| -------------- | ---------------- | --------------- |
| Cumulative Sum | Running totals   | `sum().over()`  |
| Window Frame   | Define row range | `rowsBetween()` |
| UDF            | Custom logic     | `udf()`         |

---

# Best Practices

* Always define proper **orderBy** in window functions
* Use **partitionBy** for grouped calculations
* Use **rowsBetween** for rolling metrics
* Prefer **built-in functions** over UDF
* Use **Pandas UDF** when custom logic is required at scale

---

# End Notes

These three concepts are heavily used in:

* Data Engineering pipelines
* ETL workflows
* Financial analytics
* Time-series processing
* Production-grade Spark jobs

Mastering them helps you write efficient, scalable PySpark transformations.
