# PySpark Window Functions, UDFs, Data Writing, and Spark SQL

---

## Overview

This document explains the following PySpark concepts in a **simple but detailed** way with proper examples:

* Window Functions
* `ROW_NUMBER()`
* `RANK()` vs `DENSE_RANK()`
* Cumulative Sum
* Window Frame (`rowsBetween`)
* User Defined Functions (UDF)
* Data Writing (CSV, Parquet, Table)
* Write Modes (append, overwrite, error, ignore)
* Spark SQL
* Temporary Views

All examples assume a DataFrame named **`df`**.

---

# WINDOW FUNCTIONS

## What are Window Functions?

Window functions perform calculations **across a group of rows** that are related to the current row.

Important points:

* They **do NOT reduce rows** (unlike `groupBy`).
* They return a value **for every row**.
* They work using a **Window Specification**.

We import Window as:

```python
from pyspark.sql.window import Window
```

To view the dataframe:

```python
df.display()
```

---

# ROW_NUMBER()

## Purpose

Assigns a **unique sequential number** to each row based on ordering.

```python
df.withColumn('rowCol', row_number().over(Window.orderBy('Item_Identifier'))).display()
```

### Explanation

* `row_number()` gives 1, 2, 3, 4...
* `Window.orderBy('Item_Identifier')` sorts rows first.
* Then numbering is applied.

### Use Cases

* Removing duplicates
* Top N rows
* Ranking within partitions

---

# RANK VS DENSE RANK

```python
df.withColumn('rank', rank().over(Window.orderBy(col('Item_Identifier').desc())))\
        .withColumn('denseRank', dense_rank().over(Window.orderBy(col('Item_Identifier').desc()))).display()
```

## Difference

| Feature                   | RANK | DENSE_RANK |
| ------------------------- | ---- | ---------- |
| Same values get same rank | Yes  | Yes        |
| Skips next rank           | Yes  | No         |

### Example

If values are:

```
100
100
90
```

| Value | Rank | Dense Rank |
| ----- | ---- | ---------- |
| 100   | 1    | 1          |
| 100   | 1    | 1          |
| 90    | 3    | 2          |

---

# WINDOW FRAME (Running Sum Example)

```python
df.withColumn('dum', sum('Item_MRP').over(
    Window.orderBy('Item_Identifier')
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)
)).display()
```

## Explanation

* `unboundedPreceding` → From first row
* `currentRow` → Up to current row
* This creates a **running total**.

---

# CUMULATIVE SUM

## Without Frame (Default Range Window)

```python
df.withColumn('cumsum', sum('Item_MRP').over(Window.orderBy('Item_Type'))).display()
```

## With Explicit Frame

```python
df.withColumn('cumsum', sum('Item_MRP').over(
    Window.orderBy('Item_Type')
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)
)).display()
```

## Total Sum Over Window

```python
df.withColumn('totalsum', sum('Item_MRP').over(
    Window.orderBy('Item_Type')
    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)
)).display()
```

### Meaning

| Frame                                     | Meaning       |
| ----------------------------------------- | ------------- |
| Unbounded Preceding → Current Row         | Running Total |
| Unbounded Preceding → Unbounded Following | Full Total    |

---

# USER DEFINED FUNCTIONS (UDF)

## What is UDF?

A **User Defined Function** allows us to create custom logic in Python and apply it to DataFrame columns.

---

## STEP 1 — Create Python Function

```python
def my_func(x):
    return x * x
```

---

## STEP 2 — Convert to UDF

```python
my_udf = udf(my_func)
```

---

## Apply UDF

```python
df.withColumn('mynewcol', my_udf('Item_MRP')).display()
```

### Result

* Creates new column
* Squares the value of `Item_MRP`

---

# DATA WRITING

## Writing CSV

```python
df.write.format('csv')\
        .save('/FileStore/tables/CSV/data.csv')
```

---

## APPEND MODE

Adds new data without deleting old data.

```python
df.write.format('csv')\
        .mode('append')\
        .save('/FileStore/tables/CSV/data.csv')
```

Alternative syntax:

```python
df.write.format('csv')\
        .mode('append')\
        .option('path','/FileStore/tables/CSV/data.csv')\
        .save()
```

---

## OVERWRITE MODE

Deletes old data and writes new data.

```python
df.write.format('csv')\
.mode('overwrite')\
.option('path','/FileStore/tables/CSV/data.csv')\
.save()
```

---

## ERROR MODE

Throws error if path already exists.

```python
df.write.format('csv')\
.mode('error')\
.option('path','/FileStore/tables/CSV/data.csv')\
.save()
```

---

## IGNORE MODE

Does nothing if data already exists.

```python
df.write.format('csv')\
.mode('ignore')\
.option('path','/FileStore/tables/CSV/data.csv')\
.save()
```

---

# PARQUET

Columnar storage format (faster and compressed).

```python
df.write.format('parquet')\
.mode('overwrite')\
.option('path','/FileStore/tables/CSV/data.csv')\
.save()
```

---

# SAVE AS TABLE

Creates a Spark managed table.

```python
df.write.format('parquet')\
.mode('overwrite')\
.saveAsTable('my_table')
```

---

```python
df.display()
```

---

# SPARK SQL

## Create Temporary View

```python
df.createTempView('my_view')
```

This allows SQL queries on DataFrame.

---

## Run SQL Query (Notebook SQL Cell)

```sql
%sql

select * from my_view where Item_Fat_Content = 'Lf'
```

---

## Run SQL Query Using Spark Session

```python
df_sql = spark.sql("select * from my_view where Item_Fat_Content = 'Lf'")
```

---

## Display Result

```python
df_sql.display()
```

---

# Summary

This README covered:

* Window functions and ranking
* Running totals using window frames
* Difference between rank and dense rank
* Creating and using UDFs
* Writing data in CSV and Parquet
* Different write modes
* Saving as table
* Using Spark SQL with temporary views

These concepts are **core building blocks** for:

* Data Engineering
* ETL Pipelines
* Big Data Analytics
* Production Spark Workflows
