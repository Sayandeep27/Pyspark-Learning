# PySpark Data Handling & Transformations – Practical Guide

---

## Overview

This guide explains important **PySpark DataFrame operations** used in real-world data engineering and analytics workflows:

* Handling Null Values
* Dropping Nulls
* Filling Nulls
* Split and Indexing
* Explode
* GroupBy Aggregations
* Collect List
* Pivot
* When–Otherwise (Conditional Columns)
* Joins (Inner, Left, Right, Anti)

All concepts are explained in **simple language** with examples.

---

# Handling Nulls

Null values represent **missing or unknown data**.

In PySpark, we can either:

* **Drop** rows containing nulls
* **Fill** null values with some default value

---

# Dropping Nulls

## 1. Drop rows where all columns are null

```python
df.dropna('all').display()
```

### Meaning

Remove only those rows where **every column is null**.

If even one column has data, the row is kept.

---

## 2. Drop rows where any column has null

```python
df.dropna('any').display()
```

### Meaning

If **any column** contains null, that row is removed.

This is a strict cleaning method.

---

## 3. Drop nulls only for specific column

```python
df.dropna(subset=['Outlet_Size']).display()
```

### Meaning

Remove rows where **Outlet_Size** is null.

Other columns may still contain nulls.

---

## View Original Data

```python
df.display()
```

Used to check data before or after cleaning.

---

# Filling Nulls

Instead of removing rows, we can **replace null values**.

## 1. Fill nulls in all columns

```python
df.fillna('NotAvailable').display()
```

### Meaning

All null values across the DataFrame become **"NotAvailable"**.

---

## 2. Fill nulls only for specific column

```python
df.fillna('NotAvailable', subset=['Outlet_Size']).display()
```

### Meaning

Only the **Outlet_Size** column nulls are replaced.

---

# SPLIT and Indexing

Used when a column contains **combined text values**.

---

## SPLIT

```python
df.withColumn('Outlet_Type', split('Outlet_Type', ' ')).display()
```

### What Happens

* Splits text using space
* Converts column into an **array**

Example:

```
"Supermarket Type1" → ["Supermarket", "Type1"]
```

---

## Indexing After Split

```python
df.withColumn('Outlet_Type', split('Outlet_Type', ' ')[1]).display()
```

### Meaning

Extract the **second word** from the split result.

Index starts from **0**.

---

# Explode

Used when a column contains **array values**.

---

## Step 1 – Create Array Column

```python
df_exp = df.withColumn('Outlet_Type', split('Outlet_Type', ' '))

df_exp.display()
```

---

## Step 2 – Explode Array into Rows

```python
df_exp.withColumn('Outlet_Type', explode('Outlet_Type')).display()
```

### Meaning

Each element of the array becomes a **separate row**.

---

## View Again

```python
df_exp.display()
```

---

## Check Value Presence in Array

```python
df_exp.withColumn('Type1_flag', array_contains('Outlet_Type', 'Type1')).display()
```

### Meaning

Creates a **True/False** flag if array contains "Type1".

---

# GroupBy

Used to perform **aggregations**.

---

## Scenario 1 – Sum

```python
df.groupBy('Item_Type').agg(sum('Item_MRP')).display()
```

Calculate total MRP per Item_Type.

---

## Scenario 2 – Average

```python
df.groupBy('Item_Type').agg(avg('Item_MRP')).display()
```

Find average price per Item_Type.

---

## Scenario 3 – Multiple Columns Grouping

```python
df.groupBy('Item_Type', 'Outlet_Size').agg(sum('Item_MRP').alias('Total_MRP')).display()
```

Group using two columns.

---

## Scenario 4 – Multiple Aggregations

```python
df.groupBy('Item_Type', 'Outlet_Size').agg(sum('Item_MRP'), avg('Item_MRP')).display()
```

Apply multiple calculations at once.

---

# Collect_List

Used to combine values into a **list per group**.

---

## Example Data

```python
data = [('user1','book1'),
        ('user1','book2'),
        ('user2','book2'),
        ('user2','book4'),
        ('user3','book1')]

schema = 'user string, book string'

df_book = spark.createDataFrame(data, schema)

df_book.display()
```

---

## Collect Books per User

```python
df_book.groupBy('user').agg(collect_list('book')).display()
```

### Meaning

Creates a list of books for each user.

---

## View Selected Columns

```python
df.select('Item_Type', 'Outlet_Size', 'Item_MRP').display()
```

---

# Pivot

Converts **rows into columns**.

```python
df.groupBy('Item_Type').pivot('Outlet_Size').agg(avg('Item_MRP')).display()
```

### Meaning

* Outlet_Size becomes columns
* Values = average Item_MRP

---

# When – Otherwise (Conditional Logic)

Used like **IF–ELSE**.

---

## Scenario 1 – Veg / Non-Veg Flag

```python
df = df.withColumn('veg_flag', when(col('Item_Type') == 'Meat', 'Non-Veg').otherwise('Veg'))

df.display()
```

---

## Scenario 2 – Multiple Conditions

```python
df.withColumn(
    'veg_exp_flag',
    when(((col('veg_flag') == 'Veg') & (col('Item_MRP') < 100)), 'Veg_Inexpensive')
    .when((col('veg_flag') == 'Veg') & (col('Item_MRP') > 100), 'Veg_Expensive')
    .otherwise('Non_Veg')
).display()
```

---

# Joins

Used to combine data from multiple DataFrames.

---

## Create Employee Data

```python
dataj1 = [('1','gaur','d01'),
          ('2','kit','d02'),
          ('3','sam','d03'),
          ('4','tim','d03'),
          ('5','aman','d05'),
          ('6','nad','d06')]

schemaj1 = 'emp_id STRING, emp_name STRING, dept_id STRING'

df1 = spark.createDataFrame(dataj1, schemaj1)
```

---

## Create Department Data

```python
dataj2 = [('d01','HR'),
          ('d02','Marketing'),
          ('d03','Accounts'),
          ('d04','IT'),
          ('d05','Finance')]

schemaj2 = 'dept_id STRING, department STRING'

df2 = spark.createDataFrame(dataj2, schemaj2)
```

---

## View Tables

```python
df1.display()

df2.display()
```

---

## Inner Join

```python
df1.join(df2, df1['dept_id'] == df2['dept_id'], 'inner').display()
```

### Meaning

Only matching dept_id rows appear.

---

## Left Join

```python
df1.join(df2, df1['dept_id'] == df2['dept_id'], 'left').display()
```

### Meaning

* All rows from df1
* Matching rows from df2

---

## Right Join

```python
df1.join(df2, df1['dept_id'] == df2['dept_id'], 'right').display()
```

### Meaning

* All rows from df2
* Matching rows from df1

---

## Anti Join

```python
df1.join(df2, df1['dept_id'] == df2['dept_id'], 'anti').display()
```

### Meaning

Returns rows from df1 where **no match exists** in df2.

Useful for finding missing mappings.

---

# End of Guide

This document covers the most important **day‑to‑day PySpark DataFrame operations** used in:

* Data Cleaning
* Data Transformation
* Feature Engineering
* Aggregation
* Reporting
* Data Engineering Pipelines
