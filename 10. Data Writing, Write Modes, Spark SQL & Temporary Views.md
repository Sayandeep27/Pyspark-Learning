# PySpark Guide — Data Writing, Write Modes, Spark SQL & Temporary Views

---

## Overview

This README explains the following **core PySpark concepts** in a simple and practical way:

* Data Writing (CSV, Parquet, Table)
* Write Modes (append, overwrite, error, ignore)
* Spark SQL
* Temporary Views

These concepts are very important for **real-world data engineering, ETL pipelines, analytics, and production workflows**.

---

# 1. Data Writing in PySpark

PySpark allows us to **save DataFrames** into different storage formats.

Common formats:

* CSV
* Parquet
* Table (Managed / External)

General Syntax:

```python
DataFrame.write.format("format").save("path")
```

Or

```python
DataFrame.write.csv("path")
```

---

## 1.1 Writing Data as CSV

### Syntax

```python
df.write.csv("/FileStore/output/data_csv")
```

### With Options

```python
df.write \
  .option("header", True) \
  .mode("overwrite") \
  .csv("/FileStore/output/data_csv")
```

### Important Options

| Option      | Meaning                          |
| ----------- | -------------------------------- |
| header      | Writes column names in first row |
| sep         | Custom delimiter                 |
| inferSchema | Used while reading, not writing  |

---

## 1.2 Writing Data as Parquet

**Parquet** is a columnar storage format and is the **most preferred format in big data**.

Why Parquet?

* Faster queries
* Less storage
* Supports schema
* Best for analytics

### Syntax

```python
df.write.parquet("/FileStore/output/data_parquet")
```

### With Mode

```python
df.write \
  .mode("overwrite") \
  .parquet("/FileStore/output/data_parquet")
```

---

## 1.3 Writing Data as Table

We can save a DataFrame directly as a **Spark Table**.

### Syntax

```python
df.write.saveAsTable("sales_table")
```

### With Mode

```python
df.write \
  .mode("overwrite") \
  .saveAsTable("sales_table")
```

### Types of Tables

| Type           | Description                   |
| -------------- | ----------------------------- |
| Managed Table  | Spark manages data + metadata |
| External Table | Only metadata managed         |

---

# 2. Write Modes in PySpark

Write modes control **what happens if data already exists** at the destination.

---

## 2.1 Append Mode

Adds new data to existing data.

```python
df.write.mode("append").csv("/FileStore/output/data_csv")
```

Use Case:

* Incremental loads
* Daily data ingestion

---

## 2.2 Overwrite Mode

Deletes existing data and writes fresh data.

```python
df.write.mode("overwrite").csv("/FileStore/output/data_csv")
```

Use Case:

* Full refresh
* Reprocessing pipelines

---

## 2.3 Error Mode (Default)

Throws error if data already exists.

```python
df.write.mode("error").csv("/FileStore/output/data_csv")
```

Also known as:

* `errorifexists`

Use Case:

* Prevent accidental overwrite

---

## 2.4 Ignore Mode

Does nothing if data already exists.

```python
df.write.mode("ignore").csv("/FileStore/output/data_csv")
```

Use Case:

* Safe pipelines
* Idempotent jobs

---

## Write Modes Summary

| Mode      | Behavior        | Risk Level |
| --------- | --------------- | ---------- |
| append    | Adds data       | Low        |
| overwrite | Replaces data   | High       |
| error     | Stops execution | Safe       |
| ignore    | Skips writing   | Safe       |

---

# 3. Spark SQL

Spark SQL allows us to run **SQL queries on DataFrames**.

This is useful because:

* Business users know SQL
* Easy analytics
* Works with DataFrames

---

## Steps to Use Spark SQL

### Step 1 — Create Temporary View

```python
df.createOrReplaceTempView("sales_view")
```

---

### Step 2 — Run SQL Query

```python
spark.sql("SELECT * FROM sales_view").display()
```

---

### Example Queries

```python
spark.sql("SELECT Item_Type, AVG(Item_Weight) FROM sales_view GROUP BY Item_Type").display()
```

```python
spark.sql("SELECT * FROM sales_view WHERE Item_Fat_Content = 'Regular'").display()
```

---

## When to Use Spark SQL

* Complex aggregations
* Joins
* Business reporting
* Ad-hoc analysis

---

# 4. Temporary Views (Very Important)

Temporary Views allow us to **treat a DataFrame like a SQL table**.

They exist only during the **current Spark session**.

---

## Key Idea

DataFrame → Temporary View → SQL Query

---

## Creating Temporary View

```python
df.createOrReplaceTempView("sales_view")
```

Meaning:

* If view exists → replace
* If not → create new

---

## Using the Temporary View

```python
spark.sql("SELECT * FROM sales_view").display()
```

---

## Lifetime of Temporary View

| Property    | Value        |
| ----------- | ------------ |
| Scope       | Session only |
| Storage     | Memory       |
| Persistence | Not saved    |

When Spark session ends → View is deleted.

---

## Global Temporary View (Extra Concept)

Available across sessions.

```python
df.createOrReplaceGlobalTempView("sales_global_view")
```

Access using:

```python
spark.sql("SELECT * FROM global_temp.sales_global_view").display()
```

---

## Temp View vs Table

| Feature     | Temp View | Table              |
| ----------- | --------- | ------------------ |
| Storage     | Memory    | Disk               |
| Lifetime    | Session   | Permanent          |
| Performance | Fast      | Depends on storage |
| Use Case    | Analysis  | Production         |

---

# End Notes

These concepts are **fundamental building blocks** of any PySpark project:

* Data Writing is used in ETL pipelines
* Write Modes prevent data loss
* Spark SQL enables SQL-based analytics
* Temporary Views bridge DataFrames and SQL

Mastering these will help in:

* Data Engineering Roles
* Big Data Projects
* Production Pipelines
* Analytics Systems

---
