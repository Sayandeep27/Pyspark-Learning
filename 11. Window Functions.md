# PySpark Window Functions â€” Super Easy Guide (Beginner Friendly)

---

## What is a Window Function?

**Window Functions** in PySpark allow you to perform calculations across a set of rows that are related to the current row **without reducing the number of rows**.

In simple words:

> Window function = Do calculation on a group of rows, but still keep every row.

Unlike `groupBy()`, which **combines rows**, window functions **keep all rows** and just add new calculated columns.

---

## Real Life Example

Suppose you have sales data of multiple stores.

You want:

* Rank products by sales
* Find running total
* Compare each row with previous row
* Find highest sale per store

Window functions help in all these cases.

---

## Why Not Use groupBy?

| groupBy                                      | Window Function                     |
| -------------------------------------------- | ----------------------------------- |
| Reduces rows                                 | Keeps all rows                      |
| Gives aggregated result                      | Gives row-level + aggregated result |
| Cannot compare with previous/next row easily | Can compare rows                    |

---

## Basic Syntax

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import *
```

Create Window:

```python
windowSpec = Window.partitionBy("column").orderBy("column")
```

Use Window:

```python
df.withColumn("newColumn", function().over(windowSpec))
```

---

## Sample Dataset

```python
data = [
    ("Store1", "Rice", 100),
    ("Store1", "Wheat", 200),
    ("Store1", "Sugar", 150),
    ("Store2", "Rice", 300),
    ("Store2", "Wheat", 100)
]

columns = ["Store", "Item", "Sales"]

df = spark.createDataFrame(data, columns)

df.show()
```

---

# Types of Window Operations

---

## 1. ROW_NUMBER()

Gives **unique sequence number** to each row inside a partition.

```python
from pyspark.sql.functions import row_number

windowSpec = Window.partitionBy("Store").orderBy(col("Sales").desc())

(df
 .withColumn("row_number", row_number().over(windowSpec))
 .show())
```

**Use Case**

* Find top product per store

---

## 2. RANK()

Gives rank but **skips numbers if tie happens**.

```python
from pyspark.sql.functions import rank

(df
 .withColumn("rank", rank().over(windowSpec))
 .show())
```

Example ranking:

| Sales | Rank |
| ----- | ---- |
| 300   | 1    |
| 200   | 2    |
| 200   | 2    |
| 100   | 4    |

---

## 3. DENSE_RANK()

Same as rank but **does not skip numbers**.

```python
from pyspark.sql.functions import dense_rank

(df
 .withColumn("dense_rank", dense_rank().over(windowSpec))
 .show())
```

Example:

| Sales | Dense Rank |
| ----- | ---------- |
| 300   | 1          |
| 200   | 2          |
| 200   | 2          |
| 100   | 3          |

---

## RANK vs DENSE_RANK

| Feature             | RANK | DENSE_RANK |
| ------------------- | ---- | ---------- |
| Skip numbers on tie | Yes  | No         |
| Continuous ranking  | No   | Yes        |

---

## 4. LAG()

Gets value from **previous row**.

```python
from pyspark.sql.functions import lag

(df
 .withColumn("prev_sales", lag("Sales", 1).over(windowSpec))
 .show())
```

**Use Case**

* Compare current sale with previous sale

---

## 5. LEAD()

Gets value from **next row**.

```python
from pyspark.sql.functions import lead

(df
 .withColumn("next_sales", lead("Sales", 1).over(windowSpec))
 .show())
```

---

## 6. Running Total (Cumulative Sum)

```python
from pyspark.sql.functions import sum

windowSpec2 = Window.partitionBy("Store").orderBy("Sales").rowsBetween(Window.unboundedPreceding, Window.currentRow)

(df
 .withColumn("running_total", sum("Sales").over(windowSpec2))
 .show())
```

---

## 7. Average Over Window

```python
from pyspark.sql.functions import avg

(df
 .withColumn("avg_sales", avg("Sales").over(windowSpec))
 .show())
```

---

## Important Window Components

### 1. partitionBy()

Creates groups (like groupBy but keeps rows).

Example:

* Partition by Store
* Each store processed separately

---

### 2. orderBy()

Defines order inside partition.

Example:

* Highest sales first

---

### 3. rowsBetween()

Defines frame range.

Common values:

* `Window.unboundedPreceding`
* `Window.currentRow`
* `Window.unboundedFollowing`

---

## Most Asked Interview Questions

### Difference Between groupBy and Window Function

| groupBy           | Window                  |
| ----------------- | ----------------------- |
| Aggregates data   | Adds analytical column  |
| Rows reduced      | Rows preserved          |
| No row comparison | Supports row comparison |

---

### When Should You Use Window Functions?

Use when you need:

* Ranking
* Running totals
* Previous/Next comparison
* Top N per category
* Deduplication using row_number

---

## Real Industry Use Cases

* Sales leaderboard
* Customer transaction analysis
* Fraud detection pattern
* Stock price trend
* Employee performance ranking

---

## Complete Example in One Flow

```python
from pyspark.sql.window import Window
from pyspark.sql.functions import *

windowSpec = Window.partitionBy("Store").orderBy(col("Sales").desc())

result = (df
          .withColumn("row_number", row_number().over(windowSpec))
          .withColumn("rank", rank().over(windowSpec))
          .withColumn("dense_rank", dense_rank().over(windowSpec))
          .withColumn("prev_sales", lag("Sales",1).over(windowSpec))
          .withColumn("next_sales", lead("Sales",1).over(windowSpec))
          )

result.show()
```

---

## One Line Memory Trick

> Window Function = **group calculation without collapsing rows**

---

## Common Mistakes Beginners Make

* Forgetting `orderBy()` when ranking
* Using groupBy instead of window
* Not defining partition
* Wrong frame in running total

---

## Final Summary

* Window functions perform calculations across related rows.
* They **do not reduce rows**.
* Used for ranking, comparison, cumulative metrics.
* Very important for **Data Engineering and Analytics**.

---

**End of README**
