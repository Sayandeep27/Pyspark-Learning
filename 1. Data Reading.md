# PySpark Data Reading in Databricks (JSON & CSV)

---

## Project Title

**Data Reading in PySpark using Databricks (JSON and CSV Files)**

---

## Overview

This project explains how to **read data files (JSON and CSV)** into **PySpark DataFrames** inside the **Databricks** environment. It demonstrates how to configure file reading options such as schema inference, headers, multiline handling, and how to verify file paths using Databricks utilities.

The content covers:

* Reading **JSON** files
* Reading **CSV** files
* Using **Databricks File System (DBFS)** utilities
* Understanding important Spark read options
* Viewing data using Databricks display functionality

---

## Problem Statement

In real-world data engineering and analytics workflows, data is stored in different formats like **JSON** and **CSV**. The problem is to:

* Load these files into Spark efficiently
* Ensure correct data types
* Handle headers properly
* Confirm file availability and paths
* Visualize the loaded data for validation

---

## Objectives

1. Read a JSON file into a Spark DataFrame.
2. Read a CSV file into a Spark DataFrame.
3. Automatically detect schema (data types).
4. Use header information correctly.
5. Handle JSON formatting using multiline option.
6. Verify files inside Databricks storage.
7. Display the loaded dataset in tabular format.

---

## Dataset / Data Source (if any)

Files are stored inside **Databricks DBFS**:

* `/FileStore/tables/drivers.json`
* `/FileStore/tables/BigMart_Sales.csv`

These files are accessed using Spark's `load()` function.

---

## System Architecture / Workflow

**Workflow:**

1. Store files inside **DBFS (Databricks FileStore)**.
2. Use **spark.read** to specify file format.
3. Configure reading options.
4. Load the file into a **DataFrame**.
5. Validate files using **dbutils.fs.ls**.
6. Display data using **display()**.

---

## Step-by-Step Implementation

### 1. Reading JSON File

```python
df_json = spark.read.format('json') \
    .option('inferSchema', True) \
    .option('header', True) \
    .option('multiLine', False) \
    .load('/FileStore/tables/drivers.json')

 df_json.display()
```

---

### 2. Checking Files in Databricks Storage

```python
dbutils.fs.ls('/FileStore/tables/')
```

---

### 3. Reading CSV File

```python
df = spark.read.format('csv') \
    .option('inferSchema', True) \
    .option('header', True) \
    .load('/FileStore/tables/BigMart_Sales.csv')

 df.display()
```

---

## Tech Stack

* **Apache Spark (PySpark)**
* **Databricks Platform**

---

## Tools & Libraries

* Spark Session (`spark`)
* Databricks Utilities (`dbutils`)
* DBFS (Databricks File System)

---

## Installation

Prerequisites:

* Databricks Workspace
* Cluster with Spark enabled
* Files uploaded to `/FileStore/tables/`

---

## Project Structure

```
DBFS
└── FileStore
    └── tables
        ├── drivers.json
        └── BigMart_Sales.csv
```

---

## Code Explanation

### JSON Reading Code Breakdown

* `spark.read.format('json')`

  * Specifies that the file type is **JSON**.

* `option('inferSchema', True)`

  * Spark automatically detects **data types** (number, string, date, etc.).

* `option('header', True)`

  * Typically used for CSV. For JSON, it does not significantly affect behavior.

* `option('multiLine', False)`

  * Each JSON record is expected to be in **one line**.
  * If JSON records span multiple lines, this should be set to `True`.

* `load('/FileStore/tables/drivers.json')`

  * Loads the file from Databricks storage.

* `df_json.display()`

  * Displays the DataFrame as a table in Databricks.

---

### Databricks File Check

* `dbutils.fs.ls('/FileStore/tables/')`

  * Lists all files inside the folder.
  * Used to:

    * Verify file existence
    * Confirm correct filename
    * Validate correct path

**Meaning:**

"Show me what files are inside this folder."

---

### CSV Reading Code Breakdown

* `spark.read.format('csv')`

  * Specifies that the file type is **CSV**.

* `inferSchema = True`

  * Spark guesses column data types.

* `header = True`

  * First row contains column names.

* `load('BigMart_Sales.csv')`

  * Reads the file from storage.

* `df.display()`

  * Shows the data in table format.

---

## Key Concepts Explained

### Core Spark Reading Components

* **spark.read** → Used to read data.
* **format()** → Specifies file type (json, csv, parquet, etc.).
* **option()** → Additional configuration settings.
* **load()** → File path location.
* **display()** → Visualize the data.

---

### Important Options

| Option      | Purpose                              | Notes                                |
| ----------- | ------------------------------------ | ------------------------------------ |
| inferSchema | Automatically detect data types      | Helps avoid manual schema definition |
| header      | First row contains column names      | Mostly relevant for CSV              |
| multiLine   | Handles JSON spanning multiple lines | Use True for formatted JSON          |

---

## Examples

### Real-Life Analogy

Think **Spark** like **Excel**:

* **format** → File type (Excel, CSV, JSON)
* **header** → Column names present
* **inferSchema** → Detects if values are numbers or text
* **load** → Opening the file
* **display** → Viewing the sheet

---

## Tables (Comparisons)

### Mentioned Interview Topic (Concept Reference)

Difference between:

* `spark.read.csv()`
* `spark.read.format("csv")`
* `spark.read.load()`

*(These approaches are referenced as commonly asked in Data Engineer interviews.)*

---

## Results / Output

* JSON and CSV files are successfully loaded into **Spark DataFrames**.
* Data is displayed in **tabular format** using Databricks UI.
* Schema is inferred automatically.
* File paths are validated using DBFS utilities.

---

## Use Cases

* Data Engineering pipelines
* ETL workflows
* Data validation before transformation
* Loading raw data into Spark
* Preparing datasets for analytics and ML

---

## Advantages

* Automatic schema detection reduces manual effort.
* Supports multiple file formats.
* Easy visualization with `display()`.
* DBFS integration simplifies file management.

---

## Limitations

* `header` option is not meaningful for JSON in most cases.
* Incorrect `multiLine` setting may cause parsing errors.
* Schema inference may sometimes misinterpret data types.

---

## Future Improvements

* Define explicit schemas instead of relying on inference.
* Extend to other formats like **Parquet**, **Avro**, and **Delta**.
* Add data validation and cleaning steps after loading.
* Integrate with downstream ETL and analytics pipelines.

---

## Conclusion

This project demonstrates how to read **JSON** and **CSV** files into **PySpark DataFrames** within **Databricks**, configure essential reading options, verify file locations using DBFS utilities, and visualize data. Understanding these foundational steps is critical for building reliable data engineering workflows and preparing datasets for further processing, analytics, and machine learning.
